{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change the base path before executing code\n",
    "Base_path = \"/home/tennistetris/Documents/Data Analytics/Hall/Inter_hall\"\n",
    "data1 = \"NAV_data\"\n",
    "data2 = \"Mix\"\n",
    "Path_to_data = os.path.join(Base_path,data1,data2)\n",
    "files = os.listdir(Path_to_data)\n",
    "os.chdir(Path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Continuous_dates(Values):\n",
    "    '''\n",
    "    This function takes an intermittent pandas series indexed by date \n",
    "    and converts it to a continuous pandas series where missing values\n",
    "    are replaced by the Naive Method (prev. value)    \n",
    "    '''\n",
    "    Index  = Values.index\n",
    "    Start_date = Index[0]\n",
    "    End_date = Index[len(Index)-1]\n",
    "    Num_dates = (End_date - Start_date).days\n",
    "    Cont_dates = pd.date_range(Start_date ,End_date)\n",
    "    ts = pd.Series(np.zeros(len(Cont_dates)),index = Cont_dates)\n",
    "    i = 0;j = 0\n",
    "    while(j<len(Index)):\n",
    "        #print (i,' : ',Cont_dates[i]);print(j,' : ',Index[j])\n",
    "        if((Cont_dates[i] - Index[j]).days == 0):\n",
    "            ts[i] = Values.iloc[j] \n",
    "            i = i+1;j=j+1\n",
    "        elif((Cont_dates[i] - Index[j]).days < 0):\n",
    "            ts[i] = Values.iloc[j-1]\n",
    "            i = i+1\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.chdir(Path_to_data)\n",
    "# db = pd.read_csv('100033.csv')\n",
    "# Index = list()\n",
    "# for i in range(db.shape[0]):\n",
    "#     Index.append(dt.datetime.strptime(db.iloc[i,6],'%d-%b-%Y'))\n",
    "# NAV = pd.Series(db[db.columns[3]])\n",
    "# NAV.index = Index\n",
    "# NAV = Continuous_dates(NAV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quarterly_Returns = pd.Series(np.zeros(len(NAV)),index =NAV.index)\n",
    "# Time_lag = 91 #Since we are calculating Quaterly Returns\n",
    "#               #Each quarter is assumed to have 91 days\n",
    "# for i in range(len(NAV)):\n",
    "#     t2 = i\n",
    "#     t1 = max(0,(i-Time_lag))\n",
    "#     Quarterly_Returns[i]  = np.log(NAV[t2]/NAV[t1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List1 = ['01' , '04' , '07' , '10']\n",
    "# List2 = ['2013' , '2014' , '2015' , '2016' , '2017']\n",
    "# Quarters = list(itertools.product(List1,List2))\n",
    "\n",
    "# List3 = list()\n",
    "\n",
    "# for i in range(len(Quarters)):\n",
    "#     temp = Quarters[i]\n",
    "#     temp = temp[1]+'-'+temp[0]\n",
    "#     temp = pd.Period(temp,'Q')\n",
    "#     List3.append(temp)\n",
    "    \n",
    "# Quarters = pd.Series(List3).sort_values().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Start_date = Quarterly_Returns.index[0].to_pydatetime()\n",
    "# End_date = Quarterly_Returns.index[len(NAV)-1].to_pydatetime()\n",
    "# #Now to sort the Quarters Series so as to only contain valid quarters\n",
    "# Valid_quarters = []\n",
    "# Valid_quarters_indices = []\n",
    "# for i in range(len(Quarters)):\n",
    "#     if (Quarters[i].end_time.to_pydatetime())>=Start_date:\n",
    "#         if (Quarters[i].start_time.to_pydatetime())<=End_date:\n",
    "#             Valid_quarters.append(Quarters[i])\n",
    "# Index = pd.Series(Valid_quarters)\n",
    "# print type(Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013Q1   -0.049426\n",
      "2013Q2   -0.038899\n",
      "2013Q3   -0.025241\n",
      "2013Q4    0.108927\n",
      "2014Q1    0.059256\n",
      "2014Q2    0.184241\n",
      "2014Q3    0.129207\n",
      "2014Q4    0.093428\n",
      "2015Q1    0.115777\n",
      "2015Q2   -0.006196\n",
      "2015Q3    0.026964\n",
      "2015Q4   -0.034050\n",
      "2016Q1   -0.057974\n",
      "2016Q2    0.080383\n",
      "2016Q3    0.138917\n",
      "2016Q4   -0.012411\n",
      "2017Q1    0.042709\n",
      "2017Q2    0.095495\n",
      "2017Q3    0.079664\n",
      "2017Q4    0.046593\n",
      "Freq: Q-DEC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Avg_quarterly_returns = pd.Series(np.zeros(len(Index)),Index) \n",
    "# k = 0  # this variable is there to track the dates in NAV data\n",
    "# Quarterly_dates = Quarterly_Returns.index\n",
    "# Avg_Quarterly_Returns = list()\n",
    "# #print len(Quarterly_dates)\n",
    "# for i in range(len(Index)):\n",
    "#     #print ('i : ',i)\n",
    "#     Sum_quarterly = 0\n",
    "#     num = 0\n",
    "#     while((Quarterly_dates[k] < Index[i].start_time) or (Quarterly_dates[k]<Quarterly_dates[0])):\n",
    "#         k = k+1\n",
    "#         #print ('Quarterly_dates : ',Quarterly_dates[k])\n",
    "#         #print ('start_date',Index[i].start_time)\n",
    "#         #print ('k1 :',k)\n",
    "#     while (k < 1824) and ((Quarterly_dates[k]<=Quarterly_Returns.index[len(Quarterly_Returns)-1]) and (Quarterly_dates[k]<=Index[i].end_time)):\n",
    "#         Sum_quarterly = Sum_quarterly + Quarterly_Returns[k]\n",
    "#         k = k+1\n",
    "#         #print ('k2:',k)\n",
    "#         num = num+1\n",
    "#         #print ('num :',num)\n",
    "#     Mean = Sum_quarterly/num\n",
    "#     Avg_quarterly_returns[i] = Mean\n",
    "# print Avg_quarterly_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# #Doing zero padding of data to make downstream processing easier\n",
    "# Avg_quartery_returns_padded = pd.Series(np.zeros(len(Quarters)),Quarters)\n",
    "# Avg_quarterly_returns_index = Avg_quarterly_returns.index\n",
    "# for i in range(len(Avg_quartery_returns_padded)):\n",
    "#     if Quarters[i] in Avg_quarterly_returns_index:\n",
    "#         Avg_quartery_returns_padded[i] = Avg_quarterly_returns[i]\n",
    "#     else:\n",
    "#         Avg_quarterly_returns_padded[i] = 0\n",
    "# print len(Avg_quarterly_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tennistetris/Documents/Data Analytics/Hall/Inter_hall/NAV_quarterly\n"
     ]
    }
   ],
   "source": [
    "#Defining Write path\n",
    "\n",
    "Base_path = \"/home/tennistetris/Documents/Data Analytics/Hall/Inter_hall\"\n",
    "if not 'NAV_quarterly' in os.listdir(Base_path):\n",
    "    os.mkdir('NAV_quarterly')\n",
    "Write_path = os.path.join(Base_path,'NAV_quarterly') \n",
    "print Write_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tennistetris/Documents/Data Analytics/Hall/Inter_hall\n",
      "   Unnamed: 0  Serial_No                                          Fund_Name  \\\n",
      "0           1          1  Aditya Birla Sun Life Banking And Financial Se...   \n",
      "1           2          2  Aditya Birla Sun Life Frontline Equity Fund - ...   \n",
      "2           3          3  Aditya Birla Sun Life New Millennium Fund - Di...   \n",
      "3           4          4  Aditya Birla Sun Life Top 100 Fund - Direct Pl...   \n",
      "4           5          5              HDFC Large Cap Fund - Direct Plan (G)   \n",
      "\n",
      "         ID                         Fund_Family                  Fund_Class  \\\n",
      "0  125597.0  Aditya Birla Sun Life Mutual Fund   Sector - Banking & Finance   \n",
      "1  119528.0  Aditya Birla Sun Life Mutual Fund                    Large Cap   \n",
      "2  120539.0  Aditya Birla Sun Life Mutual Fund          Sector - Technology   \n",
      "3  119564.0  Aditya Birla Sun Life Mutual Fund                    Large Cap   \n",
      "4  130498.0                    HDFC Mutual Fund                   Large Cap   \n",
      "\n",
      "     Plan Options Riskometer CRISIL_MF_RANK        ...          Chemicals  \\\n",
      "0  Direct  Growth          H             NR        ...                0.0   \n",
      "1  Direct  Growth         MH              4        ...                0.0   \n",
      "2  Direct  Growth          H             NR        ...                0.0   \n",
      "3  Direct  Growth         MH              5        ...                0.0   \n",
      "4  Direct  Growth         MH              1        ...                0.0   \n",
      "\n",
      "  Food_Beverages  Services NonDurable  Retail_Real_estate Utilities  \\\n",
      "0            0.0       0.0        0.0                 0.0      0.00   \n",
      "1            0.0       0.0        0.0                 0.0      0.00   \n",
      "2            0.0       0.0        0.0                 0.0      0.00   \n",
      "3            0.0       0.0        0.0                 0.0      2.78   \n",
      "4            0.0       0.0        0.0                 0.0      4.01   \n",
      "\n",
      "   Telecommunication Cement   IT ManufacturingMedia  \n",
      "0                0.0    0.0  0.0                  0  \n",
      "1                0.0    0.0  0.0                  0  \n",
      "2                0.0    0.0  0.0                  0  \n",
      "3                0.0    0.0  0.0                  0  \n",
      "4                0.0    3.2  0.0                  0  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "9\n",
      "9\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "9\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "17\n",
      "20\n",
      "15\n",
      "20\n",
      "20\n",
      "9\n",
      "4\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "9\n",
      "4\n",
      "20\n",
      "9\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "9\n",
      "12\n",
      "15\n",
      "20\n",
      "20\n",
      "9\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "17\n",
      "9\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "12\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "12\n",
      "20\n",
      "17\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "9\n",
      "20\n",
      "20\n",
      "15\n",
      "4\n",
      "20\n",
      "20\n",
      "20\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "#Using itertools to initialize the list of quarters\n",
    "List1 = ['01' , '04' , '07' , '10']\n",
    "List2 = ['2013' , '2014' , '2015' , '2016' , '2017']\n",
    "Quarters = list(itertools.product(List1,List2))\n",
    "\n",
    "List3 = list()\n",
    "\n",
    "for i in range(len(Quarters)):\n",
    "    temp = Quarters[i]\n",
    "    temp = temp[1]+'-'+temp[0]\n",
    "    temp = pd.Period(temp,'Q')\n",
    "    List3.append(temp)\n",
    "    \n",
    "Quarters = pd.Series(List3).sort_values().reset_index(drop = True) #List of all quarters\n",
    "\n",
    "os.chdir(Path_to_data)\n",
    "files = os.listdir(Path_to_data)\n",
    "\n",
    "Required_files = []\n",
    "os.chdir(Base_path)\n",
    "\n",
    "#Reading the IDs of the files given in the original PS and indexing by these files\n",
    "\n",
    "\n",
    "db_temp1 = pd.read_csv('MF_IIT_DATA_V1_EDITED.csv')\n",
    "db_temp2 = pd.read_csv('MF_IIT_DATA_V3_EDITED.csv')\n",
    "print Base_path\n",
    "\n",
    "IDs = db_temp2['ID']\n",
    "print db_temp2.head()\n",
    "Valid_IDs = list()\n",
    "#Removing NaNs\n",
    "for i in range(len(IDs)):\n",
    "    try:\n",
    "        Valid_IDs.append(int(IDs[i]))\n",
    "    except ValueError:\n",
    "        Valid_IDs.append(0)\n",
    "\n",
    "IDs = db_temp1['Id']\n",
    "db_temp1.head()\n",
    "#Removing NaNs\n",
    "for i in range(len(IDs)):\n",
    "    try:\n",
    "        Valid_IDs.append(int(IDs[i]))\n",
    "    except ValueError:\n",
    "        Valid_IDs.append(0)\n",
    "\n",
    "Valid_files = list()\n",
    "\n",
    "for file in files:\n",
    "    if int(file.split('.')[0]) in Valid_IDs:\n",
    "        Valid_files.append(file)\n",
    "\n",
    "for file in Valid_files:\n",
    "    #Reading Data\n",
    "\n",
    "    os.chdir(Path_to_data)\n",
    "    db = pd.read_csv(file)\n",
    "    \n",
    "    #Indexing the dates\n",
    "    \n",
    "    Index = list()\n",
    "    for i in range(db.shape[0]):\n",
    "        Index.append(dt.datetime.strptime(db.iloc[i,6],'%d-%b-%Y'))\n",
    "    NAV = pd.Series(db[db.columns[3]])\n",
    "    NAV.index = Index\n",
    "    NAV = Continuous_dates(NAV)  # To convert to continupus timeseries data\n",
    "    \n",
    "    #Finding the Quarterly Returns\n",
    "    \n",
    "    Quarterly_Returns = pd.Series(np.zeros(len(NAV)),index =NAV.index)\n",
    "    Time_lag = 91 #Since we are calculating Quaterly Returns\n",
    "              #Each quarter is assumed to have 91 days\n",
    "    for i in range(len(NAV)):\n",
    "        t2 = i\n",
    "        t1 = max(0,(i-Time_lag))\n",
    "        Quarterly_Returns[i]  = np.log(NAV[t2]/NAV[t1])\n",
    "    \n",
    "    #Now to sort the Quarters Series so as to only contain valid quarters\n",
    "    \n",
    "    Start_date = Quarterly_Returns.index[0].to_pydatetime()\n",
    "    End_date = Quarterly_Returns.index[len(Quarterly_Returns)-1].to_pydatetime()\n",
    "    Valid_quarters = []\n",
    "    Valid_quarters_indices = []\n",
    "    for i in range(len(Quarters)):\n",
    "        if (Quarters[i].end_time.to_pydatetime())>=Start_date:\n",
    "            if (Quarters[i].start_time.to_pydatetime())<=End_date:\n",
    "                Valid_quarters.append(Quarters[i])\n",
    "    Index = pd.Series(Valid_quarters)\n",
    "    \n",
    "    #Finding the average quarterly reports\n",
    "    \n",
    "    Avg_quarterly_returns = pd.Series(np.zeros(len(Index)),Index) \n",
    "    k = 0  # this variable is there to track the dates in NAV data\n",
    "    Quarterly_dates = Quarterly_Returns.index\n",
    "    for i in range(len(Index)):\n",
    "        Sum_quarterly = 0\n",
    "        num = 0\n",
    "        while((Quarterly_dates[k] < Index[i].start_time) or (Quarterly_dates[k]<Quarterly_dates[0])):\n",
    "            k = k+1\n",
    "        while (k < len(NAV)) and ((Quarterly_dates[k]<=Quarterly_Returns.index[len(Quarterly_Returns)-1]) and (Quarterly_dates[k]<=Index[i].end_time)):\n",
    "            Sum_quarterly = Sum_quarterly + Quarterly_Returns[k]\n",
    "            k = k+1\n",
    "            num = num+1\n",
    "        Mean = Sum_quarterly/num\n",
    "        Avg_quarterly_returns[i] = Mean\n",
    "    \n",
    "    #Adding Zero padding for efficient downstream analysis(i.e. replacing NaNs by zeros since data is not always available)   \n",
    "    \n",
    "    Avg_quarterly_returns_padded = pd.Series(np.zeros(len(Quarters)),Quarters)\n",
    "    Avg_quarterly_returns_index = Avg_quarterly_returns.index\n",
    "    for i in range(len(Avg_quartery_returns_padded)):\n",
    "        if Quarters[i] in Avg_quarterly_returns_index:\n",
    "            Avg_quarterly_returns_padded[i] = Avg_quarterly_returns[i]\n",
    "        else:\n",
    "            Avg_quarterly_returns_padded[i] = 0\n",
    "    os.chdir(Write_path)\n",
    "\n",
    "    #Writing the quarterly returns to a file indexed by Fund ID\n",
    "    \n",
    "    Avg_quarterly_returns.to_csv(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
